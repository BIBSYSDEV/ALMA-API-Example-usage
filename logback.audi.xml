<?xml version="1.0" encoding="UTF-8" ?>
<configuration debug="true">
    <contextListener class="ch.qos.logback.classic.jul.LevelChangePropagator"/>

    <!--  The variables contained in that file will be read and then defined within local scope -->
    <property file="logback-local.properties" />
 
    <property name="pattern" value="%d{dd MMM yyyy HH:mm:ss.SSS} %-5level %-15([%thread]) [%class.%method:%line] %mdc{ASPECT} %marker %replace(ContainerId=%mdc{CONTAINER_ID} -){'ContainerId= -', ''} %m%n" /> 

    <!-- <property name="pattern" value="%d{dd MMM yyyy HH:mm:ss.SSS} %-5level %-15([%thread]) [%class.%method:%line] - %mdc{ASPECT} %mdc{CONTAINER_MESSAGE} %marker %m%n" /> -->
    
	<property name="joblogpattern" value="%d{dd MMM yyyy HH:mm:ss.SSS} %-5level - %m%n" />

	<property name="smtpHost" value="smtp.bibsys.no" />


    <turboFilter class="ch.qos.logback.classic.turbo.DynamicThresholdFilter">
        <Key>LOG_LEVEL</Key>
        <DefaultThreshold>ERROR</DefaultThreshold> <!-- Set at highest. If eventLevel is less than defaultLevel, then NEUTRAL -->
        <OnHigherOrEqual>ACCEPT</OnHigherOrEqual>
        <OnLower>NEUTRAL</OnLower>
        <MDCValueLevelPair class="ch.qos.logback.classic.turbo.MDCValueLevelPair">
            <value>TRACE</value>
            <level>TRACE</level>
        </MDCValueLevelPair>
        <MDCValueLevelPair class="ch.qos.logback.classic.turbo.MDCValueLevelPair">
            <value>DEBUG</value>
            <level>DEBUG</level>
        </MDCValueLevelPair>
        <MDCValueLevelPair class="ch.qos.logback.classic.turbo.MDCValueLevelPair">
            <value>INFO</value>
            <level>INFO</level>
        </MDCValueLevelPair>
        <MDCValueLevelPair class="ch.qos.logback.classic.turbo.MDCValueLevelPair">
            <value>WARN</value>
            <level>WARN</level>
        </MDCValueLevelPair>
        <MDCValueLevelPair class="ch.qos.logback.classic.turbo.MDCValueLevelPair">
            <value>ERROR</value>
            <level>ERROR</level>
        </MDCValueLevelPair>
        <MDCValueLevelPair class="ch.qos.logback.classic.turbo.MDCValueLevelPair">
            <value>OFF</value>
            <level>OFF</level>
        </MDCValueLevelPair>
    </turboFilter>
    
    <turboFilter class="ch.qos.logback.classic.turbo.MarkerFilter">
        <Marker>SECURITY</Marker>
        <OnMatch>ACCEPT</OnMatch>
    </turboFilter>
    
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <Pattern>${pattern}</Pattern>
        </encoder>
    </appender>
    
    <logger name="net.sourceforge.cobertura" level="INFO"/>
    <logger name="com.wordnik.swagger" level="INFO"/>

    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <!-- rollover daily -->
            <fileNamePattern>/fasehome/applogs/${CONTEXT_NAME:-UNKNOWN}/%d{yyyy-MM-dd}-%d{HHmmss, aux}_${HOSTNAME}_%i.log</fileNamePattern>
            <!-- each file should be at most 100MB, keep 60 days worth of history, but at most 20GB -->
            <maxFileSize>100MB</maxFileSize>
            <!-- keep 90 days' worth of history -->
            <maxHistory>90</maxHistory>
            <!--<totalSizeCap>20GB</totalSizeCap>-->
        </rollingPolicy>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>${pattern}</pattern>
        </encoder>
    </appender>
    
    <appender name="FILE_LOGSTASH" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
            <!-- rollover daily -->
            <fileNamePattern>/fasehome/applogs/${CONTEXT_NAME:-UNKNOWN}/%d{yyyy-MM-dd}-%d{HHmmss, aux}_${HOSTNAME}_%i.json.log</fileNamePattern>
            <!-- each file should be at most 100MB, keep 60 days worth of history, but at most 20GB -->
            <maxFileSize>100MB</maxFileSize>
            <!-- keep 90 days' worth of history -->
            <maxHistory>90</maxHistory>
            <totalSizeCap>20GB</totalSizeCap>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">            
            <!--<timeZone>UTC</timeZone>-->
            <providers class="net.logstash.logback.composite.loggingevent.LoggingEventJsonProviders">
                <timestamp class="net.logstash.logback.composite.loggingevent.LoggingEventFormattedTimestampJsonProvider"/>
                <mdc class="net.logstash.logback.composite.loggingevent.MdcJsonProvider"/> <!-- MDC variables on the Thread will be written as JSON fields--> 
                <context class="net.logstash.logback.composite.ContextJsonProvider"/> <!--Outputs entries from logback's context -->
                <!--<callerData/>-->
                <version class="net.logstash.logback.composite.LogstashVersionJsonProvider"/> <!-- Logstash json format version, the @version field in the output-->
                <logLevel class="net.logstash.logback.composite.loggingevent.LogLevelJsonProvider"/>
                <loggerName class="net.logstash.logback.composite.loggingevent.LoggerNameJsonProvider"/>

                <!-- we can add some custom fields to be sent with all the log entries.-->
                <!--make filtering easier in Logstash-->
                <!--or searching with Kibana-->
                <pattern class="net.logstash.logback.composite.loggingevent.LoggingEventPatternJsonProvider">
                 <pattern>
                    {      
                    "location": "%class.%method{}\\(%file:%line{}\\)",
                    "contextName": "%contextName"
                    }
                 </pattern>
                </pattern>

                <threadName class="net.logstash.logback.composite.loggingevent.ThreadNameJsonProvider"/>
                <message class="net.logstash.logback.composite.loggingevent.MessageJsonProvider"/>

                <tags class="net.logstash.logback.composite.loggingevent.TagsJsonProvider"/> <!-- Logback Marker(s) -->
                <logstashMarkers class="net.logstash.logback.composite.loggingevent.LogstashMarkersJsonProvider"/> <!-- Useful so we can add extra information for specific log lines as Markers--> 
                <arguments class="net.logstash.logback.composite.loggingevent.ArgumentsJsonProvider"/> <!--or through StructuredArguments-->

                <stackTrace class="net.logstash.logback.composite.loggingevent.StackTraceJsonProvider">
                    <throwableConverter class="net.logstash.logback.stacktrace.ShortenedThrowableConverter">
                        <maxDepthPerThrowable>30</maxDepthPerThrowable>
                        <maxLength>2048</maxLength>
                        <shortenedClassNameLength>20</shortenedClassNameLength>
                        <exclude>sun\.reflect\..*\.invoke.*</exclude>
                        <exclude>net\.sf\.cglib\.proxy\.MethodProxy\.invoke</exclude>
                        <evaluator class="no.bibsys.logging.logstash.StackTraceEvaluator"/>
                        <!--<rootCauseFirst>true</rootCauseFirst>-->
                    </throwableConverter>
                </stackTrace>
            </providers>
        </encoder>
    </appender> 

  

	<appender name="CHECK-IN-OUT-FILE"
			  class="ch.qos.logback.core.rolling.RollingFileAppender">
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<!-- daily rollover -->
			<fileNamePattern>/fasehome/applogs/${CONTEXT_NAME:-UNKNOWN}/extra/%d{yyyy-MM-dd}-%d{HHmmss,aux}_${HOSTNAME}_%i.log
			</fileNamePattern>
			<timeBasedFileNamingAndTriggeringPolicy
					class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<!-- or whenever the file size reaches 100MB -->
				<maxFileSize>100MB</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>

			<!-- keep 90 days' worth of history -->
			<maxHistory>90</maxHistory>

		</rollingPolicy>

		<encoder>
			<pattern>%d{dd MMM yyyy HH:mm:ss.SSS},%level,%msg%n</pattern>
		</encoder>
	</appender>

	<appender name="AWS_EMAIL_LOG" class="ch.qos.logback.classic.net.SMTPAppender">
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>INFO</level>
		</filter>
		<evaluator class="ch.qos.logback.classic.boolex.OnMarkerEvaluator">
			<marker>AUDI_ORDER_NOTIFY_NB</marker>
		</evaluator>
		<discriminator class="ch.qos.logback.classic.sift.MDCBasedDiscriminator">
			<key>AWS_REQUEST_TYPE</key>
			<defaultValue>default</defaultValue>
		</discriminator>
		<smtpHost>${smtpHost}</smtpHost>
		<to>%mdc{to}</to>
		<from>audi@bibsys.no</from>
		<subject>${HOSTNAME} - %mdc{AWS_REQUEST_TYPE} - %d{yyyy-MM-dd HH:mm:ss}</subject>
		<includeCallerData>true</includeCallerData>
		<layout class="ch.qos.logback.classic.html.HTMLLayout">
			<!--  <pattern>%level%msg</pattern> -->
			<!--  <pattern>%d{dd MMM yyyy HH:mm:ss.SSS}%level%thread%mdc%logger%class%method%line%msg</pattern>  -->
			<pattern>%d{dd MMM yyyy HH:mm:ss.SSS}%level%msg</pattern>
		</layout>
		<cyclicBufferTracker class="ch.qos.logback.core.spi.CyclicBufferTracker">
			<!-- send max 256 log entries per email -->
			<bufferSize>256</bufferSize>
			<timeout>3600000</timeout>
		</cyclicBufferTracker>

	</appender>
	
	
	<appender name="AWS_EMAIL_ALERT_LOG" class="ch.qos.logback.classic.net.SMTPAppender">
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>ERROR</level>
		</filter>
		<evaluator class="ch.qos.logback.classic.boolex.OnMarkerEvaluator">
			<marker>AUDI_AWS_ALERT_NB</marker>
		</evaluator>
		<smtpHost>${smtpHost}</smtpHost>
		<to>%mdc{to}</to>
		<from>audi@bibsys.no</from>
		<subject>${HOSTNAME} - ALERT AWS - %d{yyyy-MM-dd HH:mm:ss}</subject>
		<includeCallerData>true</includeCallerData>
		<layout class="ch.qos.logback.classic.html.HTMLLayout">
			<!--  <pattern>%level%msg</pattern> -->
			<!--  <pattern>%d{dd MMM yyyy HH:mm:ss.SSS}%level%thread%mdc%logger%class%method%line%msg</pattern>  -->
			<pattern>%d{dd MMM yyyy HH:mm:ss.SSS}%level%msg</pattern>
		</layout>
		<cyclicBufferTracker class="ch.qos.logback.core.spi.CyclicBufferTracker">
			<!-- send max 256 log entries per email -->
			<bufferSize>256</bufferSize>
			<timeout>3600000</timeout>
		</cyclicBufferTracker>
	</appender>
	
	<appender name="DIGITIZATION_EMAIL_LOG" class="ch.qos.logback.classic.net.SMTPAppender">
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>INFO</level>
		</filter>
		<evaluator class="ch.qos.logback.classic.boolex.OnMarkerEvaluator">
			<marker>DIGITIZATION_EMAIL</marker>
		</evaluator>
		<discriminator class="ch.qos.logback.classic.sift.MDCBasedDiscriminator">
			<key>DIGITIZATION_DATE</key>
			<defaultValue>default</defaultValue>
		</discriminator>
		
		<smtpHost>${smtpHost}</smtpHost>
		<to>%mdc{to}</to>
		<from>audi@bibsys.no</from>
		<subject>${HOSTNAME} - DIGITIZATION EMAIL - %d{yyyy-MM-dd HH:mm:ss}</subject>
		<includeCallerData>true</includeCallerData>
		<asynchronousSending>false</asynchronousSending>
		
		<layout class="ch.qos.logback.classic.html.HTMLLayout">
			<!--  <pattern>%level%msg</pattern> -->
			<!--  <pattern>%d{dd MMM yyyy HH:mm:ss.SSS}%level%thread%mdc%logger%class%method%line%msg</pattern>  -->
			<pattern>%msg</pattern>
		</layout>
		<cyclicBufferTracker class="ch.qos.logback.core.spi.CyclicBufferTracker">
			<!-- send max 256 log entries per email -->
			<bufferSize>256</bufferSize>
			<timeout>3600000</timeout>
		</cyclicBufferTracker>
	</appender>
	
	
	
	

<!-- 
	<appender name="EMAILJOBS" class="ch.qos.logback.classic.net.SMTPAppender">
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>INFO</level>
		</filter>
		<evaluator class="ch.qos.logback.classic.boolex.OnMarkerEvaluator">
			<marker>AUDI_JOB_AUDIT</marker>
			<marker>AUDI_JOB_FAILURE</marker>
		</evaluator>
		<discriminator class="ch.qos.logback.classic.sift.MDCBasedDiscriminator">
			<key>AUDI_JOB_ID</key>
			<defaultValue>default</defaultValue>
		</discriminator>
		<smtpHost>${smtpHost}</smtpHost>
		<to>%mdc{to}</to>
		<from>audi@bibsys.no</from>
		<subject>%mdc{AUDI_JOB_ID} @ %d{yyyy-MM-dd HH:mm:ss}</subject>
		<includeCallerData>true</includeCallerData>
		<layout class="ch.qos.logback.classic.html.HTMLLayout">
			<pattern>%level%msg</pattern>
		</layout>
		<cyclicBufferTracker class="ch.qos.logback.core.spi.CyclicBufferTracker">
			<bufferSize>256</bufferSize>
			<timeout>3600000</timeout>
		</cyclicBufferTracker>

	</appender>
 -->

<!--
	<appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
	    <appender-ref ref="FILE" />
	    <includeCallerData>true</includeCallerData>
  	</appender>

	<appender name="ASYNCCONSOLE" class="ch.qos.logback.classic.AsyncAppender">
	    <appender-ref ref="CONSOLE" />
	    <includeCallerData>true</includeCallerData>
  	</appender>
-->

	
	
	  <!-- Conditional processing and JaninoEventEvaluator require the Janino library -->
    <root level="${root.level:-debug}"> <!-- override in ${PROJECT_BASEDIR}/logback-local.properties -->
        <!--  you can access system properties, logback properties or special variables like HOSTNAME and CONTEXT_NAME -->

        <if condition='"${root.log_to_console}" == "true"'>
            <then>
                <appender-ref ref="CONSOLE" />
            </then>
        </if>
        
		<appender-ref ref="AWS_EMAIL_LOG" />
		<appender-ref ref="AWS_EMAIL_ALERT_LOG" />
		<appender-ref ref="DIGITIZATION_EMAIL_LOG" />
		        
        
        <appender-ref ref="FILE_LOGSTASH" />
        <appender-ref ref="FILE" />

    </root>
	
	
	<!-- additivity=false can be added to ensure data only goes to the specific log -->
	
	
	
	<logger name="checkinout" level="INFO">
		<appender-ref ref="CHECK-IN-OUT-FILE"/>
	</logger>

	<logger name="org.glassfish.jersey" level="WARN" />
    <logger name="org.quartz" level ="WARN" />
    <logger name="org.codehaus.janino" level="WARN" />
    <logger name="com.sun.xml" level="ERROR" />
    <logger name="javax.xml" level="ERROR" />
    <logger name="java.util.logging" level="ERROR" />
    <logger name="sun.net" level="ERROR" />
    <logger name="no.bibsys.feide.FeideUtil" level="INFO" />
    <logger name="sun.rmi.runtime" level="ERROR" />
    <logger name="net.sourceforge.cobertura" level="INFO"/>
    <logger name="com.wordnik.swagger" level="INFO"/>
    <logger name="com.vaadin" level="INFO"/>
	
	
	
	
	
	
</configuration>